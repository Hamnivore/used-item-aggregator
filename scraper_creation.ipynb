{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from curl_cffi import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Sec-Fetch-Dest': 'document',\n",
    "            'Sec-Fetch-Mode': 'navigate',\n",
    "            'Sec-Fetch-Site': 'none',\n",
    "            'Sec-Fetch-User': '?1',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        })\n",
    "\n",
    "    def search(self, query):\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CraigslistScraper(Scraper):\n",
    "    def __init__(self, location='chicago'):\n",
    "        super().__init__()\n",
    "        self.base_url = f\"https://{location}.craigslist.org\"\n",
    "        self.api_url = \"https://sapi.craigslist.org/web/v8/postings/search/full\"\n",
    "\n",
    "    def search(self, query):\n",
    "        self.init_session()\n",
    "        search_path = self.perform_search(query)\n",
    "        data = self.api_request(query, search_path)\n",
    "        if data:\n",
    "            return self.extract_listings(data)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def init_session(self):\n",
    "        response = self.session.get(self.base_url, impersonate=\"chrome110\")\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Failed to initialize session: {response.status_code}\")\n",
    "\n",
    "    def perform_search(self, query):\n",
    "        search_url = f\"{self.base_url}/search/sss?query={query}\"\n",
    "        response = self.session.get(search_url, impersonate=\"chrome110\")\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Failed to perform search: {response.status_code}\")\n",
    "\n",
    "        # Extract searchPath from the response\n",
    "        match = re.search(r'var searchPath = \"([^\"]+)\";', response.text)\n",
    "        if match:\n",
    "            search_path = match.group(1)\n",
    "        else:\n",
    "            search_path = \"sss\"  # default if not found\n",
    "\n",
    "        return search_path\n",
    "\n",
    "    def api_request(self, query, search_path):\n",
    "        params = {\n",
    "            'batch': '11-0-360-0-0',\n",
    "            'cc': 'US',\n",
    "            'lang': 'en',\n",
    "            'query': query,\n",
    "            'searchPath': search_path\n",
    "        }\n",
    "\n",
    "        headers = {\n",
    "            'Accept': '*/*',\n",
    "            'Origin': self.base_url,\n",
    "            'Referer': self.base_url + '/',\n",
    "            'Sec-Fetch-Dest': 'empty',\n",
    "            'Sec-Fetch-Mode': 'cors',\n",
    "            'Sec-Fetch-Site': 'same-site',\n",
    "            'Pragma': 'no-cache',\n",
    "            'Cache-Control': 'no-cache'\n",
    "        }\n",
    "\n",
    "        response = self.session.get(\n",
    "            self.api_url,\n",
    "            params=params,\n",
    "            headers=headers,\n",
    "            impersonate=\"chrome110\"\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"Error: Status code {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "    def extract_listings(self, data):\n",
    "        listings = []\n",
    "        items = data.get('data', {}).get('items', [])\n",
    "        \n",
    "        for item in items:\n",
    "            listing = {}\n",
    "            listing['name'] = item[-1] if item else None\n",
    "            listing['price'] = item[3] if len(item) > 3 else None\n",
    "            \n",
    "            image_data = next((sublist for sublist in item if isinstance(sublist, list) and sublist and sublist[0] == 4), None)\n",
    "            if image_data:\n",
    "                listing['image_urls'] = self.construct_image_urls(image_data[1:])\n",
    "            else:\n",
    "                listing['image_urls'] = []\n",
    "            \n",
    "            url_data = next((sublist for sublist in item if isinstance(sublist, list) and sublist and sublist[0] == 6), None)\n",
    "            if url_data:\n",
    "                listing['url'] = self.construct_item_url(url_data)\n",
    "            else:\n",
    "                listing['url'] = None\n",
    "            \n",
    "            listings.append(listing)\n",
    "        return listings\n",
    "\n",
    "    def construct_item_url(self, url_data):\n",
    "        url_part = url_data[1] if len(url_data) > 1 else ''\n",
    "        return f\"{self.base_url}/{url_part}.html\"\n",
    "\n",
    "    def construct_image_urls(self, image_ids):\n",
    "        base_image_url = \"https://images.craigslist.org\"\n",
    "        return [f\"{base_image_url}/{id.split(':')[1]}_300x300.jpg\" for id in image_ids if ':' in id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EbayScraper(Scraper):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base_url = \"https://www.ebay.com/sch/i.html\"\n",
    "\n",
    "    def search(self, query):\n",
    "        params = {\n",
    "            '_nkw': query\n",
    "        }\n",
    "        response = self.session.get(self.base_url, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return self.extract_listings(response.text)\n",
    "        else:\n",
    "            print(f\"Error: Status code {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "    def extract_listings(self, html):\n",
    "        soup = bs(html, 'html.parser')\n",
    "        listings = []\n",
    "\n",
    "        # Find the unordered list with class 'srp-results'\n",
    "        ul = soup.select_one('.srp-results')\n",
    "        if ul:\n",
    "            # Find all list items with an id attribute\n",
    "            for li in ul.find_all('li', id=True):\n",
    "                listing = self.parse_listing(li)\n",
    "                if listing:\n",
    "                    listings.append(listing)\n",
    "\n",
    "        return listings\n",
    "\n",
    "    def parse_listing(self, li):\n",
    "        listing = {\n",
    "            \"name\": None,\n",
    "            \"price\": None,\n",
    "            \"image_urls\": [],\n",
    "            \"url\": None\n",
    "        }\n",
    "\n",
    "        # Extract title\n",
    "        title_elem = li.select_one('.s-item__title')\n",
    "        if title_elem:\n",
    "            listing[\"name\"] = title_elem.text.strip()\n",
    "\n",
    "        # Extract price\n",
    "        price_elem = li.select_one('.s-item__price')\n",
    "        if price_elem:\n",
    "            price_text = price_elem.text.strip()\n",
    "            price_match = re.search(r'\\$?([\\d,]+(\\.\\d{2})?)', price_text)\n",
    "            if price_match:\n",
    "                listing[\"price\"] = float(price_match.group(1).replace(',', ''))\n",
    "\n",
    "        # Extract URL\n",
    "        url_elem = li.select_one('a.s-item__link')\n",
    "        if url_elem:\n",
    "            listing[\"url\"] = url_elem['href']\n",
    "\n",
    "        # Extract image URL (just the first img element)\n",
    "        img_elem = li.select_one('img')\n",
    "        if img_elem:\n",
    "            src = img_elem.get('src') or img_elem.get('data-src')\n",
    "            if src:\n",
    "                listing[\"image_urls\"] = [src]\n",
    "\n",
    "        return listing if listing[\"name\"] else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OfferUpScraper(Scraper):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base_url = \"https://offerup.com/search?q=\"\n",
    "\n",
    "    def search(self, query):\n",
    "        response = self.session.get(self.base_url + query)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return self.extract_listings(response.text)\n",
    "        else:\n",
    "            print(f\"Error: Status code {response.status_code}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_listings(self, html):\n",
    "        soup = bs(html, 'html.parser')\n",
    "        listings = []\n",
    "\n",
    "        # Find the \"Current listings\" section\n",
    "        current_listings = soup.find('h2', string='Current listings')\n",
    "        if current_listings:\n",
    "            # Find the closest container that might hold the listings\n",
    "            listings_container = current_listings.find_next('ul')\n",
    "            \n",
    "            if listings_container:\n",
    "                for li in listings_container.find_all('li'):\n",
    "                    listing = self.parse_listing(li)\n",
    "                    if listing:\n",
    "                        listings.append(listing)\n",
    "\n",
    "        return listings\n",
    "\n",
    "    def parse_listing(self, li):\n",
    "        listing = {\n",
    "            \"name\": None,\n",
    "            \"price\": None,\n",
    "            \"image_urls\": [],\n",
    "            \"url\": None\n",
    "        }\n",
    "\n",
    "        # Extract title (first span with MuiTypography-subtitle1 class)\n",
    "        title_span = li.find('span', class_='MuiTypography-subtitle1')\n",
    "        if title_span:\n",
    "            listing[\"name\"] = title_span.text.strip()\n",
    "\n",
    "        # Extract price (look for $ sign)\n",
    "        price_span = li.find(string=re.compile(r'\\$'))\n",
    "        if price_span:\n",
    "            price_match = re.search(r'\\$?([\\d,]+(\\.\\d{2})?)', price_span)\n",
    "            if price_match:\n",
    "                listing[\"price\"] = float(price_match.group(1).replace(',', ''))\n",
    "\n",
    "        # Extract URL\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag and 'href' in a_tag.attrs:\n",
    "            listing[\"url\"] = \"https://offerup.com\" + a_tag['href']\n",
    "\n",
    "        # Extract image URL\n",
    "        img_tag = li.find('img')\n",
    "        if img_tag and 'src' in img_tag.attrs:\n",
    "            listing[\"image_urls\"] = [img_tag['src']]\n",
    "\n",
    "        return listing if listing[\"name\"] else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    scrapers = [CraigslistScraper(), EbayScraper(), OfferUpScraper()]\n",
    "    search_query = \"bike\"\n",
    "\n",
    "    for scraper in scrapers:\n",
    "        print(f\"Searching on {scraper.__class__.__name__}...\")\n",
    "        result = scraper.search(search_query)\n",
    "        \n",
    "        if result:\n",
    "            print(f\"Successfully retrieved {len(result)} listings:\")\n",
    "            for item in result[:5]:  # Print first 5 items\n",
    "                print(f\"Name: {item['name']}\")\n",
    "                print(f\"Price: ${item['price']}\" if item['price'] else \"Price: N/A\")\n",
    "                print(f\"URL: {item['url']}\")\n",
    "                print(\"-\" * 50)\n",
    "        else:\n",
    "            print(\"Failed to retrieve data\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from curl_cffi import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "\n",
    "class AsyncScraper:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Sec-Fetch-Dest': 'document',\n",
    "            'Sec-Fetch-Mode': 'navigate',\n",
    "            'Sec-Fetch-Site': 'none',\n",
    "            'Sec-Fetch-User': '?1',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        })\n",
    "\n",
    "    async def search(self, query):\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncCraigslistScraper(AsyncScraper):\n",
    "    def __init__(self, location='chicago'):\n",
    "        super().__init__()\n",
    "        self.base_url = f\"https://{location}.craigslist.org\"\n",
    "        self.api_url = \"https://sapi.craigslist.org/web/v8/postings/search/full\"\n",
    "\n",
    "    async def search(self, session, query):\n",
    "        await self.init_session(session)\n",
    "        search_path = await self.perform_search(session, query)\n",
    "        data = await self.api_request(session, query, search_path)\n",
    "        if data:\n",
    "            return self.extract_listings(data)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    async def init_session(self, session):\n",
    "        response = await session.get(self.base_url, impersonate=\"chrome110\")\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Failed to initialize session: {response.status_code}\")\n",
    "\n",
    "    async def perform_search(self, session, query):\n",
    "        search_url = f\"{self.base_url}/search/sss?query={query}\"\n",
    "        response = await session.get(search_url, impersonate=\"chrome110\")\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Failed to perform search: {response.status_code}\")\n",
    "\n",
    "        match = re.search(r'var searchPath = \"([^\"]+)\";', response.text)\n",
    "        return match.group(1) if match else \"sss\"\n",
    "\n",
    "    async def api_request(self, session, query, search_path):\n",
    "        params = {\n",
    "            'batch': '11-0-360-0-0',\n",
    "            'cc': 'US',\n",
    "            'lang': 'en',\n",
    "            'query': query,\n",
    "            'searchPath': search_path\n",
    "        }\n",
    "\n",
    "        headers = {\n",
    "            'Accept': '*/*',\n",
    "            'Origin': self.base_url,\n",
    "            'Referer': self.base_url + '/',\n",
    "            'Sec-Fetch-Dest': 'empty',\n",
    "            'Sec-Fetch-Mode': 'cors',\n",
    "            'Sec-Fetch-Site': 'same-site',\n",
    "            'Pragma': 'no-cache',\n",
    "            'Cache-Control': 'no-cache'\n",
    "        }\n",
    "\n",
    "        response = await session.get(\n",
    "            self.api_url,\n",
    "            params=params,\n",
    "            headers=headers,\n",
    "            impersonate=\"chrome110\"\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"Error: Status code {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "    def extract_listings(self, data):\n",
    "        listings = []\n",
    "        items = data.get('data', {}).get('items', [])\n",
    "        \n",
    "        for item in items:\n",
    "            listing = {}\n",
    "            listing['name'] = item[-1] if item else None\n",
    "            listing['price'] = item[3] if len(item) > 3 else None\n",
    "            \n",
    "            image_data = next((sublist for sublist in item if isinstance(sublist, list) and sublist and sublist[0] == 4), None)\n",
    "            if image_data:\n",
    "                listing['image_urls'] = self.construct_image_urls(image_data[1:])\n",
    "            else:\n",
    "                listing['image_urls'] = []\n",
    "            \n",
    "            url_data = next((sublist for sublist in item if isinstance(sublist, list) and sublist and sublist[0] == 6), None)\n",
    "            if url_data:\n",
    "                listing['url'] = self.construct_item_url(url_data)\n",
    "            else:\n",
    "                listing['url'] = None\n",
    "            \n",
    "            listings.append(listing)\n",
    "        return listings\n",
    "\n",
    "    def construct_item_url(self, url_data):\n",
    "        url_part = url_data[1] if len(url_data) > 1 else ''\n",
    "        return f\"{self.base_url}/{url_part}.html\"\n",
    "\n",
    "    def construct_image_urls(self, image_ids):\n",
    "        base_image_url = \"https://images.craigslist.org\"\n",
    "        return [f\"{base_image_url}/{id.split(':')[1]}_300x300.jpg\" for id in image_ids if ':' in id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncEbayScraper(AsyncScraper):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base_url = \"https://www.ebay.com/sch/i.html\"\n",
    "\n",
    "    async def search(self, session, query):\n",
    "        params = {'_nkw': query}\n",
    "        response = await session.get(self.base_url, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return self.extract_listings(response.text)\n",
    "        else:\n",
    "            print(f\"Error: Status code {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "    def extract_listings(self, html):\n",
    "        soup = bs(html, 'html.parser')\n",
    "        listings = []\n",
    "\n",
    "        # Find the unordered list with class 'srp-results'\n",
    "        ul = soup.select_one('.srp-results')\n",
    "        if ul:\n",
    "            # Find all list items with an id attribute\n",
    "            for li in ul.find_all('li', id=True):\n",
    "                listing = self.parse_listing(li)\n",
    "                if listing:\n",
    "                    listings.append(listing)\n",
    "\n",
    "        return listings\n",
    "\n",
    "    def parse_listing(self, li):\n",
    "        listing = {\n",
    "            \"name\": None,\n",
    "            \"price\": None,\n",
    "            \"image_urls\": [],\n",
    "            \"url\": None\n",
    "        }\n",
    "\n",
    "        # Extract title\n",
    "        title_elem = li.select_one('.s-item__title')\n",
    "        if title_elem:\n",
    "            listing[\"name\"] = title_elem.text.strip()\n",
    "\n",
    "        # Extract price\n",
    "        price_elem = li.select_one('.s-item__price')\n",
    "        if price_elem:\n",
    "            price_text = price_elem.text.strip()\n",
    "            price_match = re.search(r'\\$?([\\d,]+(\\.\\d{2})?)', price_text)\n",
    "            if price_match:\n",
    "                listing[\"price\"] = float(price_match.group(1).replace(',', ''))\n",
    "\n",
    "        # Extract URL\n",
    "        url_elem = li.select_one('a.s-item__link')\n",
    "        if url_elem:\n",
    "            listing[\"url\"] = url_elem['href']\n",
    "\n",
    "        # Extract image URL (just the first img element)\n",
    "        img_elem = li.select_one('img')\n",
    "        if img_elem:\n",
    "            src = img_elem.get('src') or img_elem.get('data-src')\n",
    "            if src:\n",
    "                listing[\"image_urls\"] = [src]\n",
    "\n",
    "        return listing if listing[\"name\"] else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncOfferUpScraper(AsyncScraper):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base_url = \"https://offerup.com/search?q=\"\n",
    "\n",
    "    async def search(self, session, query):\n",
    "        response = await session.get(self.base_url + query)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return self.extract_listings(response.text)\n",
    "        else:\n",
    "            print(f\"Error: Status code {response.status_code}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_listings(self, html):\n",
    "        soup = bs(html, 'html.parser')\n",
    "        listings = []\n",
    "\n",
    "        # Find the \"Current listings\" section\n",
    "        current_listings = soup.find('h2', string='Current listings')\n",
    "        if current_listings:\n",
    "            # Find the closest container that might hold the listings\n",
    "            listings_container = current_listings.find_next('ul')\n",
    "            \n",
    "            if listings_container:\n",
    "                for li in listings_container.find_all('li'):\n",
    "                    listing = self.parse_listing(li)\n",
    "                    if listing:\n",
    "                        listings.append(listing)\n",
    "\n",
    "        return listings\n",
    "\n",
    "    def parse_listing(self, li):\n",
    "        listing = {\n",
    "            \"name\": None,\n",
    "            \"price\": None,\n",
    "            \"image_urls\": [],\n",
    "            \"url\": None\n",
    "        }\n",
    "\n",
    "        # Extract title (first span with MuiTypography-subtitle1 class)\n",
    "        title_span = li.find('span', class_='MuiTypography-subtitle1')\n",
    "        if title_span:\n",
    "            listing[\"name\"] = title_span.text.strip()\n",
    "\n",
    "        # Extract price (look for $ sign)\n",
    "        price_span = li.find(string=re.compile(r'\\$'))\n",
    "        if price_span:\n",
    "            price_match = re.search(r'\\$?([\\d,]+(\\.\\d{2})?)', price_span)\n",
    "            if price_match:\n",
    "                listing[\"price\"] = float(price_match.group(1).replace(',', ''))\n",
    "\n",
    "        # Extract URL\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag and 'href' in a_tag.attrs:\n",
    "            listing[\"url\"] = \"https://offerup.com\" + a_tag['href']\n",
    "\n",
    "        # Extract image URL\n",
    "        img_tag = li.find('img')\n",
    "        if img_tag and 'src' in img_tag.attrs:\n",
    "            listing[\"image_urls\"] = [img_tag['src']]\n",
    "\n",
    "        return listing if listing[\"name\"] else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 23\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python38\\lib\\asyncio\\runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    scrapers = [AsyncCraigslistScraper(), AsyncEbayScraper(), AsyncOfferUpScraper()]\n",
    "    search_query = \"bike\"\n",
    "\n",
    "    async with AsyncSession(headers=AsyncScraper().headers) as session:\n",
    "        tasks = [scraper.search(session, search_query) for scraper in scrapers]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "\n",
    "    for scraper, result in zip(scrapers, results):\n",
    "        print(f\"Results from {scraper.__class__.__name__}:\")\n",
    "        if result:\n",
    "            print(f\"Successfully retrieved {len(result)} listings:\")\n",
    "            for item in result[:5]:  # Print first 5 items\n",
    "                print(f\"Name: {item['name']}\")\n",
    "                print(f\"Price: ${item['price']}\" if item['price'] else \"Price: N/A\")\n",
    "                print(f\"URL: {item['url']}\")\n",
    "                print(\"-\" * 50)\n",
    "        else:\n",
    "            print(\"Failed to retrieve data\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
